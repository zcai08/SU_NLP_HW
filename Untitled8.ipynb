{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6eb0bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c292d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "['neg', 'pos']\n",
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sentence_polarity.sents()\n",
    "print(len(sentences))\n",
    "print(type(sentences))\n",
    "print(sentence_polarity.categories())\n",
    "for sent in sentences[:4]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "607dad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n"
     ]
    }
   ],
   "source": [
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "print(len(pos_sents))\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(neg_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "275789ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "        for sent in sentence_polarity.sents(categories=cat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d6aab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')\n",
      "(['provides', 'a', 'porthole', 'into', 'that', 'noble', ',', 'trembling', 'incoherence', 'that', 'defines', 'us', 'all', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e05c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "627cc6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'but', 'with', 'film', 'this', 'for', 'its', 'an', 'movie', \"it's\", 'be', 'on', 'you', 'not', 'by', 'about', 'more', 'one', 'like', 'has', 'are', 'at', 'from', 'than', '\"', 'all', '--', 'his', 'have', 'so', 'if', 'or', 'story', 'i', 'too', 'just', 'who', 'into', 'what']\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "# look at the first 50 words in the most frequent list of words\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ba82c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "059aa5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26626bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     20.5 : 1.0\n",
      "                V_stupid = True              neg : pos    =     17.5 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     16.9 : 1.0\n",
      "                V_beauty = True              pos : neg    =     15.8 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.5 : 1.0\n",
      "               V_routine = True              neg : pos    =     14.2 : 1.0\n",
      "                  V_flat = True              neg : pos    =     14.1 : 1.0\n",
      "                V_boring = True              neg : pos    =     14.0 : 1.0\n",
      "             V_inventive = True              pos : neg    =     13.1 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     13.1 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     12.4 : 1.0\n",
      "                  V_warm = True              pos : neg    =     12.3 : 1.0\n",
      "              V_powerful = True              pos : neg    =     12.2 : 1.0\n",
      "                  V_dull = True              neg : pos    =     12.2 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     11.9 : 1.0\n",
      "                    V_90 = True              neg : pos    =     11.6 : 1.0\n",
      "              V_provides = True              pos : neg    =     11.5 : 1.0\n",
      "                V_hasn't = True              neg : pos    =     10.9 : 1.0\n",
      "                  V_ages = True              pos : neg    =     10.4 : 1.0\n",
      "                V_deftly = True              pos : neg    =     10.4 : 1.0\n",
      "           V_mesmerizing = True              pos : neg    =     10.4 : 1.0\n",
      "             V_realistic = True              pos : neg    =     10.4 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.3 : 1.0\n",
      "              V_captures = True              pos : neg    =      9.9 : 1.0\n",
      "                V_tender = True              pos : neg    =      9.7 : 1.0\n",
      "              V_touching = True              pos : neg    =      9.6 : 1.0\n",
      "            V_apparently = True              neg : pos    =      9.6 : 1.0\n",
      "              V_tiresome = True              neg : pos    =      9.6 : 1.0\n",
      "                V_unless = True              neg : pos    =      9.6 : 1.0\n",
      "                   V_wry = True              pos : neg    =      9.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a5d5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLpath = \"subjclueslen1-HLTEMNLP05.tff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6aea8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6d07f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "SL = readSubjectivity(SLpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd1781d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['strongsubj', 'adj', False, 'negative']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL['absolute']\n",
    "SL['shabby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57570cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "strength, posTag, isStemmed, polarity = SL['absolute']\n",
    "print(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05e6fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "        # count variables for the 4 classes of subjectivity\n",
    "        weakPos = 0\n",
    "        strongPos = 0\n",
    "        weakNeg = 0\n",
    "        strongNeg = 0\n",
    "        for word in document_words:\n",
    "            if word in SL:\n",
    "                strength, posTag, isStemmed, polarity = SL[word]\n",
    "                if strength == 'weaksubj' and polarity == 'positive':\n",
    "                    weakPos += 1\n",
    "                if strength == 'strongsubj' and polarity == 'positive':\n",
    "                    strongPos += 1\n",
    "                if strength == 'weaksubj' and polarity == 'negative':\n",
    "                    weakNeg += 1\n",
    "                if strength == 'strongsubj' and polarity == 'negative':\n",
    "                    strongNeg += 1\n",
    "                features['positivecount'] = weakPos + (2 * strongPos)\n",
    "                features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f6a48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [(SL_features(d, SL), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a27244f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49deab14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL_featuresets[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d437fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3a7e618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in list(sentences)[:50]:\n",
    "   for word in sent:\n",
    "     if (word.endswith(\"n't\")):\n",
    "       print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d58e7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abfa79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b65b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a166e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(NOT_featuresets[0][0]['V_always'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e866ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = NOT_featuresets[200:], NOT_featuresets[:200]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1969db36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73f01fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newstopwords = [word for word in stopwords if word not in negationwords]\n",
    "len(newstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9c44ada3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ',', 'film', 'movie', 'not', 'one', 'like', '\"', '--', 'story', 'no', 'much', 'even', 'good', 'comedy', 'time', 'characters', 'little', 'way', 'funny', 'make', 'enough', 'never', 'makes', 'may', 'us', 'work', 'best', 'bad', 'director']\n"
     ]
    }
   ],
   "source": [
    "new_all_words_list = [word for word in all_words_list if word not in newstopwords]\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "new_word_features = [word for (word,count) in new_word_items]\n",
    "print(new_word_features[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "24089144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n"
     ]
    }
   ],
   "source": [
    "featuresets2 = [(document_features(d,new_word_features), c) for (d,c) in documents]\n",
    "train_set2, test_set2 = featuresets2[1000:], featuresets2[:1000]\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train_set2)\n",
    "print (nltk.classify.accuracy(classifier2, test_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7fa0005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  V_flat = True              neg : pos    =     24.3 : 1.0\n",
      "            V_engrossing = True              pos : neg    =     21.7 : 1.0\n",
      "               V_generic = True              neg : pos    =     17.0 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     17.0 : 1.0\n",
      "               V_routine = True              neg : pos    =     16.3 : 1.0\n",
      "            V_unexpected = True              pos : neg    =     15.7 : 1.0\n",
      "             V_inventive = True              pos : neg    =     15.0 : 1.0\n",
      "                V_boring = True              neg : pos    =     14.7 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     14.3 : 1.0\n",
      "              V_haunting = True              pos : neg    =     13.0 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     13.0 : 1.0\n",
      "                  V_warm = True              pos : neg    =     13.0 : 1.0\n",
      "                    V_90 = True              neg : pos    =     13.0 : 1.0\n",
      "             V_NOTenough = True              neg : pos    =     12.3 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     12.2 : 1.0\n",
      "           V_mesmerizing = True              pos : neg    =     11.7 : 1.0\n",
      "             V_realistic = True              pos : neg    =     11.7 : 1.0\n",
      "              V_provides = True              pos : neg    =     11.4 : 1.0\n",
      "                  V_dull = True              neg : pos    =     11.1 : 1.0\n",
      "              V_captures = True              pos : neg    =     11.0 : 1.0\n",
      "                V_stupid = True              neg : pos    =     11.0 : 1.0\n",
      "                  V_ages = True              pos : neg    =     10.3 : 1.0\n",
      "              V_chilling = True              pos : neg    =     10.3 : 1.0\n",
      "              V_powerful = True              pos : neg    =     10.3 : 1.0\n",
      "             V_offensive = True              neg : pos    =     10.3 : 1.0\n",
      "                 V_stale = True              neg : pos    =     10.3 : 1.0\n",
      "              V_tiresome = True              neg : pos    =     10.3 : 1.0\n",
      "                V_unless = True              neg : pos    =     10.3 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.8 : 1.0\n",
      "                V_tender = True              pos : neg    =      9.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "54faa844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ad0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
